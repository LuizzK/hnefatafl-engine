# Copenhagen Hnefatafl - Your Questions Answered

## âœ… Q1: Can I play against myself to see if it works?

**YES!** Use the CLI interface I just created:

```bash
./venv/bin/python play.py
```

**How to play:**
- Move format: `f6-f8` (from square to square)
- Type `help` for instructions
- Type `moves` to see all legal moves
- Type `quit` to exit

**Example moves:**
```
Attackers > f2-f3    # Move attacker forward
Defenders > f6-f8    # Move king forward
Attackers > a4-d4    # Move attacker horizontally
```

The board uses algebraic notation (a-k columns, 1-11 rows), just like chess!

---

## âœ… Q2: What is left to do to start training?

You need to implement **2 more files**:

### File 1: `hnefatafl/selfplay.py` - Self-Play Engine
**Purpose**: Generate training games

**What it needs to do:**
1. Play games using MCTS + current neural network
2. Save training data: (board_state, MCTS_policy, game_outcome)
3. Manage replay buffer (store last ~500k positions)
4. Support parallel game generation (multiple games at once)

**Key functions needed:**
```python
class SelfPlayWorker:
    def play_game(model, num_simulations=800):
        """Play one self-play game, return training data"""
        # Use MCTS to select moves
        # Store (state, policy, outcome) for each move
        # Return list of training examples

    def generate_games(model, num_games=100):
        """Generate multiple games in parallel"""
        # Create batch of games
        # Use GPU efficiently
        # Return all training data
```

**Estimated complexity**: ~200-300 lines
**Time**: 1-2 days

### File 2: `hnefatafl/train.py` - Training Pipeline
**Purpose**: Train the neural network from self-play data

**What it needs to do:**
1. **Training loop**:
   - Generate self-play games with current model
   - Sample mini-batches from replay buffer
   - Train neural network (gradient descent)
   - Save checkpoints regularly

2. **Model evaluation**:
   - Pit new model vs current best (play 100 games)
   - If new wins >55%, replace current best
   - Track ELO ratings

3. **Monitoring**:
   - Log losses to TensorBoard
   - Track win rates, game lengths
   - Save training curves

**Key functions needed:**
```python
class Trainer:
    def __init__(self, model, optimizer):
        self.replay_buffer = []
        self.best_model = model

    def training_iteration(self):
        """One iteration of training"""
        # 1. Generate self-play games
        games = generate_games(self.best_model, num_games=1000)

        # 2. Add to replay buffer
        self.replay_buffer.extend(games)

        # 3. Train new model
        new_model = self.train_network(self.replay_buffer)

        # 4. Evaluate
        if self.evaluate(new_model, self.best_model):
            self.best_model = new_model

    def train_network(self, data, epochs=10):
        """Train network on data"""
        for epoch in range(epochs):
            for batch in get_batches(data):
                states, target_policies, target_values = batch

                # Forward pass
                policy_logits, value_pred = model(states)

                # Compute loss
                loss = compute_loss(policy_logits, value_pred,
                                  target_policies, target_values)

                # Backward pass
                loss.backward()
                optimizer.step()

    def evaluate(self, new_model, old_model, num_games=100):
        """Play games between models, return True if new wins"""
        wins = 0
        for _ in range(num_games):
            winner = play_game(new_model, old_model)
            if winner == new_model:
                wins += 1
        return wins / num_games > 0.55
```

**Estimated complexity**: ~400-500 lines
**Time**: 2-3 days

---

## âœ… Q3: What about the failing test case?

**FIXED!** âœ… All 22 tests now pass!

The issue was that the test was checking if pieces can pass through the empty throne, but had a piece blocking the path after the throne. I fixed the test to properly clear the path.

**Verification:**
```bash
./venv/bin/pytest tests/test_game.py -v
```

**Result:** `22 passed in 0.20s` âœ…

---

## âœ… Q4: How do you verify the NN and MCTS are working?

I created two verification scripts:

### Verify Neural Network:
```bash
./venv/bin/python verify_network.py
```

**What it tests:**
- âœ… Network creation (22M parameters)
- âœ… Forward pass (input â†’ policy + value)
- âœ… Real game state encoding
- âœ… Loss computation (policy loss + value loss)
- âœ… Backward pass (gradient computation)
- âœ… Model save/load (checkpoints)

**Expected output:**
```
âœ“ Model created with 21,973,345 parameters
âœ“ Forward pass successful
âœ“ Value predictions in valid range [-1, 1]
âœ“ Loss computation successful
âœ“ Backward pass successful
âœ“ ALL TESTS PASSED!
```

### Verify MCTS:
```bash
./venv/bin/python verify_mcts.py
```

**What it tests:**
- âœ… Basic MCTS without neural network (random rollouts)
- âœ… MCTS with neural network guidance
- âœ… MCTS node operations (expand, update, UCB)
- âœ… Full game simulation (play 20 moves)
- âœ… Temperature parameter (exploration vs exploitation)
- âœ… Performance benchmark

**Expected output:**
```
âœ“ MCTS initialized with 50 simulations
âœ“ Search completed in X seconds
âœ“ Selected move is legal
âœ“ Node expanded with N children
âœ“ Played 20 moves successfully
âœ“ ALL TESTS PASSED!
```

---

## Quick Start Guide

### 1. Test the Game Engine
```bash
# Run all unit tests
./venv/bin/pytest tests/test_game.py -v

# Play against yourself
./venv/bin/python play.py
```

### 2. Verify Components
```bash
# Verify neural network
./venv/bin/python verify_network.py

# Verify MCTS
./venv/bin/python verify_mcts.py
```

### 3. What's Working Now
- âœ… Complete game engine (all Copenhagen rules)
- âœ… Neural network architecture (ResNet + policy/value heads)
- âœ… MCTS implementation (PUCT algorithm)
- âœ… 22/22 unit tests passing
- âœ… Can play games manually via CLI

### 4. What You Need to Implement

**Priority 1: Self-Play Engine** (`selfplay.py`)
- ~200-300 lines
- Generates training data by playing games
- Most important for getting training started

**Priority 2: Training Pipeline** (`train.py`)
- ~400-500 lines
- Trains network from self-play data
- Includes model evaluation and checkpointing

**Priority 3: GUI** (`gui.py`) [OPTIONAL]
- ~300-400 lines with Pygame
- Nice to have, but not required for training
- Can be added later

---

## Minimal Training Script Template

Here's a simple version to get you started:

```python
# train_minimal.py
from hnefatafl.game import HnefataflGame
from hnefatafl.network import create_model, PolicyValueLoss
from hnefatafl.mcts import MCTS
import torch

# Create model
model = create_model()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = PolicyValueLoss()

# Training loop
for iteration in range(1000):
    print(f"Iteration {iteration}")

    # 1. Generate self-play games
    training_data = []
    for game_num in range(10):  # 10 games per iteration
        game = HnefataflGame()
        mcts = MCTS(model, num_simulations=100)

        game_data = []
        while not game.is_game_over():
            # Get MCTS policy
            move, policy = mcts.search(game)

            # Store (state, policy) for training
            state = game.encode_state()
            game_data.append((state, policy))

            # Make move
            game.make_move(move)

        # Add game outcome to all positions
        outcome = 1.0 if game.get_winner() == game.current_player else -1.0
        for state, policy in game_data:
            training_data.append((state, policy, outcome))

    # 2. Train on data
    for epoch in range(5):
        for state, target_policy, target_value in training_data:
            # Convert to tensors
            state_t = torch.FloatTensor(state).unsqueeze(0)
            policy_t = torch.FloatTensor(target_policy).unsqueeze(0)
            value_t = torch.FloatTensor([[target_value]])

            # Forward pass
            policy_logits, value_pred = model(state_t)

            # Compute loss
            loss, _, _ = loss_fn(policy_logits, value_pred, policy_t, value_t)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # 3. Save checkpoint
    if iteration % 10 == 0:
        torch.save(model.state_dict(), f'model_iter_{iteration}.pt')
        print(f"Saved checkpoint at iteration {iteration}")
```

---

## Expected Training Timeline

Based on AlphaZero papers and similar implementations:

**Hardware: Single GPU (NVIDIA)**
- Initial random â†’ coherent play: **6-12 hours**
- Coherent â†’ competent play: **1-3 days**
- Competent â†’ strong play: **1-2 weeks**
- Strong â†’ master level: **2-4 weeks**

**Hardware: CPU only (NOT RECOMMENDED)**
- Everything will be **10-50x slower**
- Initial coherent play: **3-7 days**
- Strong play: **Months**

**Recommendation**: Use Google Colab (free GPU) or AWS/Azure GPU instances for training.

---

## Summary

### âœ… What's Done:
1. **Game engine** - fully working, all rules implemented
2. **Neural network** - tested and verified
3. **MCTS** - tested and verified
4. **CLI interface** - play games manually
5. **All tests passing** - 22/22 âœ…

### ðŸš§ What You Need:
1. **selfplay.py** - generate training games (1-2 days)
2. **train.py** - training loop (2-3 days)

### ðŸ“Š Total Implementation Status:
- **Done**: ~75% of MVP
- **Remaining**: ~25% (just training pipeline)
- **Lines of code written**: ~2,800
- **Lines remaining**: ~600-800

---

## Questions?

Try things out and let me know if you have issues! The foundation is solid - you just need the training loop now.

Good luck! ðŸš€
